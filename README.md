# Gemma 3 Fine-tuning for NER & Relationship Extraction

This project demonstrates a complete pipeline for fine-tuning **Gemma 3-1B-IT** to perform Named Entity Recognition (NER) and Relationship Extraction using synthetic data generated by the Gemini API.

## Project Workflow

1.  **Synthetic Data Generation**: Using `gemini-api.py` to generate high-quality NER triplets.
2.  **Dataset Preparation**: Converting raw JSON into the specialized instruction format for Gemma 3.
3.  **PEFT/LoRA Fine-tuning**: Using `trl` and `peft` to fine-tune the model on a single GPU (or Mac MPS).

## Key Components

- `gemini-api.py`: Script to generate synthetic training data using `gemini-flash-lite-latest` via the OpenAI-compatible Gemini API.
- `entities.json`: The generated dataset containing input sentences and their corresponding entity/relationship JSON outputs.
- `finetune.ipynb`: A Jupyter notebook performing Supervised Fine-Tuning (SFT) using LoRA.
- `gemma-ner-lora/`: Directory containing the fine-tuned LoRA adapters.

## Getting Started

### Prerequisites

- Python 3.12+
- [uv](https://docs.astral.sh/uv/) (Dependency Manager)
- Google Gemini API Key

### Installation

1.  Clone the repository:
    ```bash
    git clone https://github.com/mukulboro/llm-blog-post-examples.git
    cd llm-blog-post-examples
    ```

2.  Install dependencies using `uv`:
    ```bash
    uv sync
    ```

3.  Configure your environment:
    ```bash
    export GEMINI_API_KEY="your-api-key-here"
    ```

## Usage

### 1. Generating Synthetic Data
Run the generation script to populate `entities.json`:
```bash
uv run gemini-api.py
```

### 2. Fine-tuning the Model
Open `finetune.ipynb` in your favorite editor (VS Code, Jupyter, etc.) and run the cells. The notebook is configured to automatically detect and use Apple Silicon (MPS), CUDA, or CPU. It will save the LoRA adapters to `gemma-ner-lora/`.

### 3. Serving with vLLM
You can serve the fine-tuned model using [vLLM](https://github.com/vllm-project/vllm).

```bash
vllm serve google/gemma-3-1b-it \
  --enable-lora \
  --lora-modules ner=gemma-ner-lora \
  --tokenizer gemma-ner-lora
```

### 4. Inference
Once the server is running, you can test it using `test_vllm.py`:

```bash
uv run test_vllm.py
```

Or via `curl`:

```bash
curl http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "ner",
    "prompt": "Extract entities and relationships from the text below as JSON.\n\nInput: Mukul is writing a blog on how to train LLMs\n\nJSON Output:\n",
    "max_tokens": 200,
    "temperature": 0.1
  }'
```

## Project Structure

- `gemini-api.py`: Script to generate synthetic training data via Gemini API.
- `entities.json`: Generated dataset for NER and Relationship Extraction.
- `finetune.ipynb`: Jupyter notebook for PEFT/LoRA fine-tuning on Gemma 3.
- `gemma-ner-lora/`: Fine-tuned LoRA adapter weights.
- `test_vllm.py`: Script to test the vLLM server.

