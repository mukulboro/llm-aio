{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d63b7efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b46ca8",
   "metadata": {},
   "source": [
    "# Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e5830de1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8425,\n",
       " {'text': 'Extract entities and relationships from the text below as JSON.\\n\\nInput: The new processor manufactured by Intel significantly improves the performance of the latest Macbook Pro.\\n\\nJSON Output:\\n{\"entities\": [\"processor\", \"Intel\", \"Macbook Pro\"], \"relationships\": [[\"processor\", \"MANUFACTURED_BY\", \"Intel\"], [\"processor\", \"IMPROVES_PERFORMANCE_OF\", \"Macbook Pro\"]]}<eos>'})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"entities.json\", \"r\") as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "processed_data = []\n",
    "\n",
    "for item in raw_data:\n",
    "    input_text = item.get(\"input\", \"\")\n",
    "    output_json = item.get(\"output\", {})\n",
    "    \n",
    "    response_str = json.dumps(output_json, ensure_ascii=False)\n",
    "    \n",
    "    full_text = (\n",
    "        f\"Extract entities and relationships from the text below as JSON.\\n\\n\"\n",
    "        f\"Input: {input_text}\\n\\n\"\n",
    "        f\"JSON Output:\\n{response_str}<eos>\"\n",
    "    )\n",
    "    \n",
    "    processed_data.append({\"text\": full_text})\n",
    "\n",
    "dataset = Dataset.from_list(processed_data)\n",
    "\n",
    "len(dataset), dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1623f14",
   "metadata": {},
   "source": [
    "# Training Device Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9b813065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "MODEL_ID = \"google/gemma-3-1b-it\" \n",
    "OUTPUT_DIR = \"gemma-ner-lora\"\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "    torch_dtype = torch.bfloat16 \n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    torch_dtype = torch.bfloat16\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    torch_dtype = torch.float32\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfd8dcf",
   "metadata": {},
   "source": [
    "# Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c42f729f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "tokenizer.padding_side = \"right\" # Fix for fp16 training\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device_map=device, # Auto-moves to MPS/CUDA\n",
    "    use_cache=False    # Disable cache for training\n",
    ")\n",
    "\n",
    "# LoRA allows us to fine-tune only a tiny fraction of parameters\n",
    "peft_config = LoraConfig(\n",
    "    r=16,                   \n",
    "    lora_alpha=32,          \n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[         # Gemma specific target modules\n",
    "        \"q_proj\", \n",
    "        \"k_proj\", \n",
    "        \"v_proj\", \n",
    "        \"o_proj\", \n",
    "        \"gate_proj\", \n",
    "        \"up_proj\", \n",
    "        \"down_proj\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf65eb2",
   "metadata": {},
   "source": [
    "# Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a9968264",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding EOS to train dataset: 100%|██████████| 8425/8425 [00:00<00:00, 113598.26 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 8425/8425 [00:00<00:00, 12593.91 examples/s]\n",
      "Truncating train dataset: 100%|██████████| 8425/8425 [00:00<00:00, 1465475.52 examples/s]\n",
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    }
   ],
   "source": [
    "training_args = SFTConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_length=512,                \n",
    "    packing=False,\n",
    "    \n",
    "    # Standard Training Args\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=10,\n",
    "    max_steps=100,\n",
    "    save_strategy=\"no\",\n",
    "    optim=\"adamw_torch\",\n",
    "    fp16=False,\n",
    "    bf16=True if device == \"mps\" or device == \"cuda\" else False,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,     \n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    args=training_args\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdaae8f",
   "metadata": {},
   "source": [
    "# Train and Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "35900971",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 1}.\n",
      "/Users/mukulboro/vivasoft/blog_post/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 02:52, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.082400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.545700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.404200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.378600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.393900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.353700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.334100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.365200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.327000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.328600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd61f59e",
   "metadata": {},
   "source": [
    "# Load Model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0cad053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, OUTPUT_DIR)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b3baa3",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "70567f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract entities and relationships from the text below as JSON.\n",
      "\n",
      "Input: Mukul is writing a blog on how to train LLMs\n",
      "\n",
      "JSON Output:\n",
      "{\"entities\": [\"Mukul\", \"blog\", \"LLMs\"], \"relationships\": [[\"Mukul\", \"WRITING\", \"blog\"], [\"blog\", \"ABOUT\", \"LLMs\"]]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_input = \"Mukul is writing a blog on how to train LLMs\"\n",
    "prompt = f\"Extract entities and relationships from the text below as JSON.\\n\\nInput: {test_input}\\n\\nJSON Output:\\n\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs, \n",
    "    max_new_tokens=200, \n",
    "    do_sample=True, \n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
